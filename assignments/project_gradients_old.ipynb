{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Gradients and Solvers\n",
    "\n",
    "## Machine Learning, Fall 2018\n",
    "\n",
    "### Names: [Your name here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you'll perform the following tasks:\n",
    "\n",
    "1. [Explore gradients and contour plots](#Gradients-and-Contour-Plots),\n",
    "2. [Define a `Variable` class](#The-Variable-class) which is used to compute the gradient of an \"arbitrary\" function,\n",
    "3. [Test out your `Variable` class](#A-basic-test-of-your-Variable-class) by plotting some more gradients on contour plots, and \n",
    "4. [Build a `LogisticRegression` class](#The-LogisticRegression-class) using your `Variable` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard import statements:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients and Contour Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by understanding how contour plots work.  These are 2D representations of functions of two variables (what some people might call \"3D functions\").  You may be familiar with [contour maps](http://sitesmedia.s3.amazonaws.com/creekconnections/files/2014/09/topomap.jpg); a contour map is simply a contour plot of the function \n",
    "\n",
    "$$f(\\text{lattitude}, \\text{longitude}) = \\left<\\text{height of the Earth's surface above sea level at that point}\\right>.$$ \n",
    "\n",
    "More generally, the contours in a plot are the paths of same height.  Thus, traveling perpendicularly to a contour means traveling \"straight up or down the mountain\".  Here's a first attempt at making a contour plot for a function which somewhat resembles the top of a mountain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return 1.2 - 0.2*x**2 - 0.3*y**2 + 0.1 * x * y - 0.25 * x\n",
    "\n",
    "x = np.arange(-8,8,.1)\n",
    "y = np.arange(-8,8,.1)\n",
    "\n",
    "# maybe a first guess is:\n",
    "z = f(x,y)\n",
    "\n",
    "try:\n",
    "    plt.contour(x, y, z);\n",
    "except Exception as e:\n",
    "    print(\"Exception was raised: \\n\", type(e).__name__, \": \", e, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't work, because this only plugs values into $f$ which are on the *diagonal*, that is, those values in the $xy$-plane where $x=y$.  In order to make this work, we need a function value over every point in our grid, i.e. every pair of $x$- and $y$-values in the square.  This is why the value for z must be 2D, because its indices `i` and `j` are the indices of its $x$- and $y$-values of its point.  That is, `z[i,j] = f(x[i], y[j])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better:\n",
    "z = np.array([[f(x[i],y[j]) for i in range(x.shape[0])] for j in range(y.shape[0])])\n",
    "### Make sure you understand the above line!\n",
    "\n",
    "try:\n",
    "    plt.contour(x, y, z, cmap='Spectral');\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works, but that line defining `z` sure was a slog.  This is where the helper function `np.meshgrid` comes into play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X,Y)\n",
    "\n",
    "# Convenience for printing \n",
    "iters = [('x', x), ('X', X), ('y', y), ('Y', Y), ('z', z) , ('Z', Z)]\n",
    "\n",
    "for name, var in iters:\n",
    "    print(\"The shape of {} is {}\".format(name, var.shape))\n",
    "\n",
    "if (z == Z).all():\n",
    "    print(\"\\nThese ways work the same\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `np.meshgrid` is used to build this `Z` (which is the same as our manually-built `z`).  But what are `X` and `Y`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure you understand what the following code is doing!  It will help with understanding X and Y.\n",
    "\n",
    "for name, var in iters[:4]:\n",
    "    try:\n",
    "        snipped = var[:6, :6]\n",
    "    except:\n",
    "        snipped = var[:6]\n",
    "    print('Beginning of {}:\\n{}'.format(name, snipped), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EFDDFF; padding: 10px\">\n",
    "<p>Describe what you're seeing:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so now that we know how to plot contour plots of functions of 2 variables, let's explore adding some **gradients** to the plot.  First we define a helper function, used to help us draw the arrows on contour plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_arrow(a, b, c, d, ax=None):\n",
    "    \"\"\" Draw an arrow on a plot.\n",
    "    \n",
    "    params:\n",
    "        a, b -- coordinates of the base of the arrow\n",
    "        c, d -- vector corresponding to the arrow (starting at the origin)\n",
    "        ax   -- the matplotlib Axes object on which to draw the arrow.  If none, set to current Axes object.\n",
    "    \"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    v_0 = np.array([a,b])\n",
    "    v_1 = np.array([c,d]) + v_0\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    color='k')\n",
    "    plt.annotate('', v_1, v_0, arrowprops=arrowprops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the contour plot as well as some gradient vectors\n",
    "x = np.arange(-8,8,.1)\n",
    "y = np.arange(-8,8,.1)\n",
    "\n",
    "X, Y = np.meshgrid(x,y)\n",
    "Z = f(X,Y)\n",
    "\n",
    "plt.contour(x, y, Z, cmap='winter')\n",
    "\n",
    "# Draw some gradients\n",
    "draw_arrow(2, -4, -1.45,  2.6)\n",
    "draw_arrow(-2, -2, 0.35, 1)\n",
    "draw_arrow(4, 4, -1.45, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style=\"background-color: #EFDDFF; padding: 10px;\">\n",
    "<p> Okay, so now here's your task. The gradient of the above function happens to be:\n",
    "$$\\nabla f(x,y) = \\left<-.4 x + .1 y - .25, -.6 y + .1 x\\right>.$$ \n",
    "Create a function `draw_gradient` that, given a point $(x,y)$, draws the gradient of $f$ at that point.  Then, recreate the plot above so that your `draw_gradient` method gets called for a handful of randomly-generated points (so that you can rerun the cell and get a new handful of points each time).</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EFDFDF; padding: 10px;\">\n",
    "\n",
    "<h3> Rules for Gradients:</h3>\n",
    "\n",
    "<p>As we have seen, the gradient is a function that points us \"up the hill\", in the direction of steepest increase of a function $f$.  What does that mean?  It's important to keep in mind that if you have a function $f$ then the gradient of $f$, written as $\\nabla f$, is a different function that takes in points from the same space $f$ does and outputs a vector which points up toward the direction of steepest increase at that point.</p>\n",
    "\n",
    "<p>Below are all the rules that we will need for our gradients.  In a multivariable calculus class, you would study and possibly even prove these statements.  For us, we'll take them as our programming requirements: the gradient is simply a recursive function with a bunch of rules.  **Do not feel like you need to memorize these for my class, or understand where they come from. I will never ask you to compute the gradient of a function by hand on a test.**</p>\n",
    "\n",
    "<ul>\n",
    "<li> If $x_i$ is the $i^{\\text{th}}$ independent variable, then $\\nabla x_i = \\left<0,\\ldots,0,1,0,\\ldots, 0\\right>$, where the $1$ is in the $i^{\\text{th}}$ slot.  Another way to say this is the following.  Suppose my function $f$ just picked out the $i^{\\text{th}}$ variable: $f(x_1, \\ldots, x_p)=x_i$, for some $i$.  Then the gradient of $f$ would be length 1 and would point in the $x_i$-direction.</li>\n",
    "<li> The gradient is **linear**: </li><ul>\n",
    " <li> If $f(x_1, \\ldots, x_p) = w_1+w_2$, then $\\nabla f = \\nabla w_1 + \\nabla w_2$, and </li>\n",
    " <li> If $f(x_1, \\ldots, x_p) = c \\cdot w$, then $\\nabla f = c \\cdot \\nabla w$.</li>\n",
    " <li> (As an extension of the first point: if $f(x_1, \\ldots, x_p) = \\sum_{i=1}^n w_i$, then $\\nabla f = \\sum_{i=1}^n\\nabla w_i$.)</li>\n",
    " </ul>\n",
    "<li> The gradient has a **power rule**: </li>\n",
    " <ul>\n",
    " <li> If $f(x_1, \\ldots, x_p) = w^n$ then $\\nabla f = n w^{n-1} \\cdot \\nabla w$, </li>\n",
    " </ul>\n",
    "<li> The gradient has a **product rule**: </li>\n",
    " <ul>\n",
    " <li> If $f(x_1, \\ldots, x_p) = w_1\\cdot w_2$ then $\\nabla f = w_1 \\cdot \\nabla w_2 + w_2 \\cdot \\nabla w_1$,</li>\n",
    " </ul>\n",
    "<li> The gradient does have a *difference rule* and a *quotient rule*, but you can just define yours using the facts that $w_1 - w_2 = w_1 + (-1) \\cdot w_2$ and $\\dfrac{w_1}{w_2} = w_1 \\cdot (w_2)^{-1}$, respectively.</li>\n",
    "<li> The gradient has a **chain rule**, which manifests itself in the following ways (the only ones we'll need):</li>\n",
    " <ul>\n",
    " <li> If $f(x_1, \\ldots, x_p) = e^w$ then $\\nabla f = e^w \\cdot \\nabla w$,</li>\n",
    " <li> If $f(x_1, \\ldots, x_p) = \\ln(w)$ then $\\nabla f = \\frac{1}{w} \\cdot \\nabla w$, </li>\n",
    " <li> There are lots of others, but we won't need them for now. </li>\n",
    " </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The `Variable` class\n",
    "\n",
    "Your main task in the project is to make a `Variable` class.  The `Variable` class is a node in the computational graph.  It should have the following attributes and methods:\n",
    "\n",
    "* a class attribute `wengert` which is a list that keeps track of all the Variable objects in the order of a Wengert list.  There are fancy technical definitions and requirements of such a list, but for you, it should just be a list that holds all your `Variable` objects (so in the constructor, you should append `self` to the list).  You may get less milage out of this list than I did, depending on your implimentation, but I found it helpful to have it.\n",
    " * a class method which resets the wengert list to an empty list.  This allows you to redefine the expression if you mess up. Google `@staticmethod` to see the notation.\n",
    "* an attribute `inputs`, a list that records which inputs the current `Variable` objects takes.  This could highly leverage your wengert list.\n",
    "* an `__init__` method,\n",
    "* an `eval_` method that allows you to evaluate the variable at a given value of the primitive variables,\n",
    "* a `grad` method that will compute return the gradient at a given value.  Here, you should use a numpy array. This might sound scary, but the main reason is so that you can compute with them nicely, later (see my implementation of a `grad` method below).  For example, to do this for $f(x_1, x_2, x_3) = x_1$, that `Variable`'s gradient function should `return np.array([1,0,0])`\n",
    "* appropriate methods/functionality to calculate the gradients and function evaluation for all the gradients defined above.\n",
    "\n",
    "As an example, if I were to say:\n",
    "\n",
    "```\n",
    ">>> x = Variable()\n",
    ">>> y = Variable()\n",
    ">>> z = x + y\n",
    "```\n",
    "\n",
    "This will create a new `Variable` class for `z`, and initialize it appropriately, and give it the ability to take its gradient.  The way we can implement this is by defining a *magic* method called `__add__`.  Magic methods in Python are what give it such a great functional API.  Here's an example `__add__` method, based on how I set up my constructor.  Yours might be a little bit different:\n",
    "\n",
    "```\n",
    "    def __add__(self, other):\n",
    "        \"\"\" Defines the functionality of the `+` operator. \"\"\"\n",
    "        if isinstance(other, (int, float)):\n",
    "            \"\"\" The derivative of a constant is zero. \"\"\"\n",
    "            return Variable(inputs=[self],\n",
    "                            eval_=lambda *values: other + self.eval_(*values), \n",
    "                            grad= self.grad)\n",
    "        elif isinstance(other, Variable):\n",
    "            \"\"\" The differential operator is linear. \"\"\"\n",
    "            return Variable(inputs=[self, other],\n",
    "                            eval_=lambda *values: self.eval_(*values) \n",
    "                                  + other.eval_(*values),\n",
    "                            grad= lambda *values: self.grad(*values) \n",
    "                                  + other.grad(*values))\n",
    "        else:\n",
    "            return NotImplemented\n",
    "```\n",
    "\n",
    "See [this link](http://www.diveintopython3.net/special-method-names.html#acts-like-number) to learn about the magic methods you will need to implement.\n",
    "\n",
    "The theory here is that you create the \"ugly\" `x.add(y)`-ish code once and only once, and then when you use the code later it's much more readable: `x + y`.  You'll need to do something like this for all the operations discussed above in the [gradients section](#-Rules-for-Gradients:).  The exception is for functions like `exp` and `log` (and if I wanted you to do other things, like `sin`, `cos`, `tan`, etc.).  Those will need to be class methods.  They should still return a new `Variable` object.  See below for the API that they should support.\n",
    "\n",
    "Other methods you need to implement:\n",
    "\n",
    "* While you do need to have an `eval_` method, if you also implement the `__call__` method (make it just call `eval_`) then you can replace code of the form `w.eval_(4, 2, 8.5)` with `w(4, 2, 8.5)`, which is both cleaner and looks like function notation (which is desirable because the `w`'s really can be thought of as functions of the independent variables).\n",
    "* You might want to implement the `__repr__` method so that you can print out your variables (as in, just `print(x)`).  It would be especially nice if printing out a `Variable` object gave you a graph traversal of its `inputs`.  It's okay here if it looks like a tree, *i.e.* nodes get repeated.  It would be a bit too much work to make the true graph visual happen.\n",
    "* You will need to implement the methods `__radd__` and `__rmul__`, _etc._ that give your `__add__` and `__mul__` method more functionality.  This allows expressions like `2 + x` instead of always requiring that a user use the `Variable` object first: `x + 2`.  In your research, you may see `__iadd__` and `__imul__`, and those you _should_ skip.\n",
    "\n",
    "Here are some resources that may help you along the way:\n",
    "* A good explanation on [Automatic Differentiation](https://pdfs.semanticscholar.org/be3d/17df872d41465dabda2fc9a9a61394658a1a.pdf) (the name of this process).  It's probably much more information than you need.\n",
    "* There's always [Wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation).\n",
    "* Another take on [operator overloading in Python](http://blog.teamtreehouse.com/operator-overloading-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## A \"complete\" collection of uses that demonstrates the API \n",
    "Suppose my function is: \n",
    "\n",
    "$$f(x_1, x_2, x_3) = e^{x_1 + {x_2}^2} + 3 \\cdot \\ln(27 - x_1\\cdot x_2 \\cdot x_3).$$\n",
    "\n",
    "Then the following code would build and test my function:\n",
    "\n",
    "```\n",
    ">>> from Variable import exp, log\n",
    ">>> x_1 = Variable()\n",
    ">>> x_2 = Variable()\n",
    ">>> x_3 = Variable()\n",
    ">>> z = exp(x_1 + x_2**2) + 3 * log(27 - x_1 * x_2 * x_3)\n",
    ">>> # Evaluate the function at the point (x_1, x_2, x_3) = (3, 1, 7):\n",
    ">>> z(3, 1, 7)\n",
    "59.9734284408284\n",
    ">>> # Determine the gradient of the function at the point (x_1, x_2, x_3) = (3, 1, 7):\n",
    ">>> z.grad(3, 1, 7)\n",
    "array([ 51.09815003,  98.69630007,  -1.5       ])\n",
    ">>> print(x_1 + x_2**2)\n",
    "+(\n",
    " 1: <input 1>\n",
    " 2: ^2(\n",
    "  <input 2>\n",
    " )\n",
    ")\n",
    "```\n",
    "Two notes: \n",
    "1. I had to do some pencil-and-paper math on that gradient, let me know if it's not what you got!\n",
    "2. The print-out is optional, but helpful!  You can make it say whatever you want.  Here, I made it list the node type, then indent the inputs of that node by one, recursively.  It almost looks a bit like [prefix notation](https://en.wikipedia.org/wiki/Polish_notation).\n",
    "\n",
    "## Some thoughts on coding process\n",
    "\n",
    "* I recommend that you don't actually define the class in a single cell in this notebook.  It's going to get a bit big, and it's going to be difficult to work on it together if it's in a cell on a jupyter notebook.  I believe that a better strategy for designing this class is to have a Python file `variable.py`, located in the same folder as this notebook and opened in your favorite text editor that contains your `Variable` class, and then just rerun a cell like this to reimport it here after saving over there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from variable import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then, when you change something and want to check out how it works, just rerun the above cell.\n",
    "* This also makes sharing code via something like github a lot easier!\n",
    "* I _highly recommend_ you begin by making everything other than gradients work, and then add gradients afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put whatever test code you want here.  I'm curious to see how you tested it \n",
    "# out, so the rest of this section won't be graded in the usual \"no long printouts\", \n",
    "# grumbly ol' Dr. Z kind of way (though do let me know what you're attempting to do, \n",
    "# and whether it's working!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `LogisticRegression` class\n",
    "\n",
    "Now that you have the class needed for creating a gradient, it's time to put it to use.  Construct a `LogisticRegression` class that serves as a logistic regression model.  It should support the standard Scikit-Learn API:\n",
    "\n",
    "```\n",
    ">>> X,y = <some dataset consisting numpy arrays in the standard form>\n",
    ">>> model = LogisticRegression()\n",
    ">>> model.fit(X, y)\n",
    ">>> X_test, y_true = <some test dataset in the standard form>\n",
    ">>> y_preds = model.predict(X_test)\n",
    ">>> from sklearn.metrics import accuracy_score\n",
    ">>> accuracy_score(y_true, y_preds)\n",
    "```\n",
    "\n",
    "Use your `Variable` class in the `fit` method of your class (or wherever else seems appropriate).  Remember, you're taking the gradient of the cost function, and the cost function has as its inputs the parameters of your model.  So you should have a `Variable` for every model parameter.  You **don't** need to support multi-class predictions (as in, you can assume your output variable `y` is binary), and it is **optional** to support a multidimensional `X` (as in, perhaps my dataset has 7 predictors/features, I want to be able to use your class to fit my data).\n",
    "\n",
    "**Note**: I recognize that you could just take the partial derivative (or google it), but that's not the point of this project!  The goal is to struggle through both creating and using your `Variable` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Good luck!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
